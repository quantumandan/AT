## Algorithmic Topology With Applications (DRAFT MANUSCRIPT)

## Abstract:

Using the Hamiltonian evolution operator to model a Turing machine's successor function, the infinite tape requirement is replaced by an infinite time requirement, and an algebraic construction is presented by which formal language choice pins the vector spaces underlying a quiver theoretic representation of paths through a program's state space. This permits a mathematically consistent description of computation as the flow of information through a dynamical system. From this perspective, algorithms may be reasoned about independent of implementation, and equivalent implementations can be smoothly transformed from one to another via diffeomorphism. With the concept of flows, or sets of graph theoretic paths, we algebraically extend the quiver path algebra with results from Lie, Noether, and Hopf. A bi-algebraic structure emerges, playing an important role in the asymptotic complexity analysis of an algorithm's refactorings. The Church-Turing Thesis is then re-stated in the language of non-commutative differential geometry, the space of halting programs defines an exterior algebra, and computational complexity is shown to be an implicit feature of the underlying computational algebra, necessitating a subtler theory of asymptotic analysis when restricted to those Turing machines subject to conservation of energy.

Many questions in mathematics, theoretical physics and computer science, and applied physics could be put to rest by direct construction through a sufficiently tractable model of physically realizable universal Turing Machines. Despite the robust mathematical tool set of modern theoretical physics, it is not immediately straightforward to go from algorithm to differential equation; however, by discarding redundant state information about a classical system through the extrinsic, or category theoretical approach, computational invariants are constructived as fixed points of multi-linear coordinate transformations between vector spaces. These transformations are tensors and the aforementioned invariant corresponds to an inductively emergent differential symmetry group generated by a Noetherian conservation law on the phase space of a vector field over a manifold.

A program's execution is thus interpreted as the flow of information, or after a Wick rotation, the dissipation of entropy across the state manifold for which the Turing transfer function is embedded. Given an analytic theory of computation by thermalization, plus a recipe for constructing Turing machines as a dynamical system in a conservative potential, a variety of theoretical and practical applications are presented for the categorization, development, and exploitation of advanced algorithms and topological materials; with additional implications for fundamental physics.

# Table of Contents

## Part 1: Summary
0. Overview
1. Paradox As A Laboratory
2. Computing With Dynamical Systems
3. Information As Deviation From Random
4. What Is Intrinsic Complexity?
5. The Need For Topology
6. Topological Invariants Of Dynamical Systems
7. Conservation Laws In Dynamical Systems With Differential Symmetries
8. Using Duality To Resolve Paradox
9. A Rose By Any Other Name
10. The Duality Gap
11. Chern Polynomials Encode Intrinsic Complexity
12. Intrinsic Complexity Is A Conserved Quantity
13. Interlude On the Absurdity of P=NP (and why it doesn't)

## Part 2: Mathematical Preliminaries

14. Abstract Nonsense And The Lambda Calculus
15. Constructing A Vector Space From A Formal Language
16. Adjacency and Walk Matrix generate a Turing monoid's Transition space
17. Algebraic Statistics and Algebraic Varieties of Programming Languages
18. The Lie Algebras of Turing Manifolds

## Part 3: Applications

### Computer Science
19. Analyzing The Relationship Of An Implementation's Cyclomatic Complexity To An Algorithm's Intrinsic Complexity
20. Flow Algebra For Database Schema Analysis of Foreign Key Relationships
21. The Fundamental Limits Of Currying

### Materials Science
22. Classifying High Entropy Alloys By Their Intrinsic Complexity
23. Self Organizing Maps For Predictive Design of Superparamagnetic HEA

### Quantum Computing
24. Algorithmic Topology As A Foundation For Solid State Quantum Computing
25. Computing With Waste Heat -- Thermodynamic HEA Spintronic Computers

### Information Theory
26. Fundamental Limits On The Entanglement Density Of Near Macroscopic Thermodynamic Systems
27. Quantum Gravity Is A Theory Of Computation

# Summary

I argue that an asymptotic complexity hierarchy based on algebraic invariants, not simply an algebra's arithmetic, is a necessary feature of a mathematically and physically consistent theory of an algorithm's absolute computational complexity.

## 0. Overview

To deal with complexity independent of an algorithm's implementation requires category theory to embed via canonical injection the monoid of classically polynomial-time algorithms $\mathbf{P}$ into the groupoid of all algorithms expressible as power series, call it $\mathbf{A_P}$, where the set of all algorithms $\mathbf{A}$ is given by $\mathbf{P} \sub \mathbf{A_P} \sub \mathbf{A}$. Now **let every algorithm be identified with the complexity class to which it's implementation algebraically belongs**.

While there are uncountably infinite refactorings of any given $\mathbf{P}$ algorithm, proving a result in $\mathbf{A_P}$ is often simpler thanks to homotopy methods on prolongated jet spaces by way of renormalization group theory; and this reduction in complexity can be understood in terms of holographic interpolation theory and the Hasse principle for algebraic groups -- a process of simplification analytically elucidated by the invariance of the Dixmier trace to choice of inner product, aka our "measure of complexity". Furthermore, the space of algebraically expressible algorithms $\mathbf{A_P}$ is a universal cover of $\mathbf{P}$, thus a result true for all members of $\mathbf{A_P}$ will be true for all members of $\mathbf{P}$.

With the above in mind, in the algorithmic topology, the elements of $\mathbf{A_P}$ form an Almost Finite (AF) polynomial ring for a given Turing machine's successor function on the input data, which necessarily contains any $\mathbf{P}$ refactorings as a compact subspace. We construct a bijection from $\mathbf{P}$ to the rational numbers $\mathbb{Q}$ and then use the transitivity of poset operators, duality, and Dedekind cuts to mirror the Peano construction of $\mathbb{R}$ and algebraically complete $\mathbf{P} \sub \mathbf{A_P}$ with $\mathbf{HARD_{AF}}$, necessarily forming a new complexity class defined by the Turing machines that do not halt after countably infinite iterations of their successor function. In this new hierarchy, polynomially-expressible algorithms that halt in less than or equal to a countably infinite number of applications of their Turing successor function belong to $\mathbf{A_P}$.


* Traditional measures of computational complexity do not rigorously distinguish between **polynomial-time** $\mathbf{P}$ vs **polynomially-expressible** $\mathbf{A_P}$ solutions if they are proportional by a constant

* This results in a Russels/Banach-Tarski-esque paradox when you let an arbitrary Turing machine run long enough.

* The mathematical machinery is drawn from renormalization group flows of rearrangement invariant dynamical systems under the action of the infinite symmetric group $S_\infty$.

* By rearrangement invariant it is meant that there exists a Lie algebraic theory of infinitessimal refactorings which alter an algorithm without changing its complexity.

* This is the basis for a rigorous theory of "equipotentials" of complexity using symmetric polynomials.

* Framework is given rigour and analycity in the context of non-perturbative 2D Liouville Quantum Gravity

* No asymptotically consistent, classically $\mathbf{P}$ implementation exists despites a polynomially-expressible solution to an $\mathbf{NP_{HARD}}$ problem.

* Noether's Theorem, in the context of Chaitin, saves the day with an explanation.

## **Motivating Case**

There exists a deterministic formula to solve the Traveling Salesman problem for an arbitrary digraph $G=(V,E)$ by computing the Hamiltonian cycle space of $G$ as a matrix polynomial, using a clever choice of algebra and only canonical matrix multiplication with naive complexity $\mathcal{O(n^3)}$.

### **A Crash Course In Flow Algebra**

Flows are an extension of a directed graph's path algebra to include sets of paths.

Let $V=\{1,2,3\}$ and $E=\{(1,2), (2,3), (3,2), (3,1)\}$ be the vertex and edge sets defining directed graph $G=(V,E)$. For edges $a=(1,2)$ and $b=(2,3)$, the product is given by $a \times b \mapsto (1,2)(2,3)$. In other words, multiplication of two edges maps to a longer path in the space of walks generated by the concatenation of head to tail matching paths. Should the concatenated edges fail to form a contiguous walk on the graph, the product is $0:=\empty$. Consider the  following,

* Let $c=(3, 2)$ then the product $c\times a=(3,2)\times(1,2)= 0$.

* The concatenated path $a \times b = (1,2)(2,3)$ can be rewritten as $(1,2,3)$ without loss of information.

* $a+b =:\{a,b\}$, and $(a+b)^2=a^2 + ab + b^2$ which simplifies to $(a+b)^2=ab=\{(1,2,3)\}$.

* $a+b+c=\{a,b,c\}$, and $(a+b+c)^2=a^2 + b^2 + c^2 + ab + cb$ which simplifies to $ab+cb=\{(1,2,3), (3,2,3)\}$

For $2^V$ and $2^E$ the powersets of vertices and edges respectively, note that $2^E \subset 2^V$ so the powerset of vertices covers the powerset of edges. We may furthermore define the push and pull operators,

#### **Push (outflow)**

For $\alpha \in 2^V$ the push mapping $\delta_+:2^V \to 2^E$ takes a directed graph's path and yields all successor edges,

$\delta_+(\alpha)=\sum_{j \in \epsilon_+ (\hat{h}\alpha)}(\hat{h}\alpha,j)$

where the head operator $\hat{h}:(\alpha_0, ..., \alpha_m)\mapsto\alpha_m$ projects $\alpha$ to its terminal vertex and,

$\epsilon_+(i)=\{j : (i,j) \in E\}$

#### **Pull (inflow)**

For $\alpha \in 2^V$ the pull mapping $\delta_-:2^V \to 2^E$ takes a directed graph's path and yields all predecessor edges,

$\delta_-(\alpha)=\sum_{j \in \epsilon_- (\hat{t}\alpha)}(j,\hat{t}\alpha)$

where the tail operator $\hat{t}:(\alpha_0, ..., \alpha_m)\mapsto\alpha_0$ projects $\alpha$ to its initial vertex and, 

$\epsilon_-(i)=\{j : (j, i) \in E\}$

Similar to the topological concept of an epsilon ball, we define the oriented epsilon neighborhood of a vertex as, $\epsilon = (\epsilon_-, \epsilon_+)$

### THEOREM:
$\hat{r}\delta_+=\delta_-$ and $\hat{r}\delta_-=\delta_+$

### PROOF:

Let $x\in\Xi$ be a path, then $x=(x_1,...,x_n)$ then, $\delta_-(x) = (\mathbf{1}\wedge x)\epsilon_+(x)$ and $\hat{r}\delta_-= \hat{r}(\mathbf{1}\wedge x)\epsilon_+(x)$ but $\hat{r}\epsilon_+(x)=\epsilon_-(x)$, which implies that $\delta_-(x)=\hat{r}\delta_+(x)$. The proof follows similarly for $\delta_-(x)$

### Calculation of a toy cycle space

Now let $d=(3,1) \in E$ such that $E=\{a,b,c,d\}$. The walk matrix is just the adjacency matrix with every 1 replaced by the vertex pair representated as (row, column) coordinates.

$W_G={\begin{bmatrix}0&a&0\\0&0&b\\d&c&0\end{bmatrix}}$

$W_G^2={\begin{bmatrix}0&0&ab\\bd&0&0\\0&da&cb\end{bmatrix}}$

$W_G^3={\begin{bmatrix}0&a&0\\0&0&b\\d&c&0\end{bmatrix}}\times {\begin{bmatrix}0&0&ab\\bd&0&0\\0&da&cb\end{bmatrix}}={\begin{bmatrix}abd&0&0\\0&bda&0\\cbd&0&dab\end{bmatrix}}$

$\operatorname{Tr}W_G^3=abd+bda+dab \cong [abd]=[(1,2,3,1)]$ is the conjugacy class of cyclic permutations of the Hamiltonian cycle space for graph $G$.

No non-Archimedian algebra can satisfy the above requirements -- however, exact exterior algebras are nilpotent of degree 2, and this is the prototypical implementation of a flow algebra in physics. The cycle $[(1,2,3,1)]=[(1,2)(2,3)(3,1)] = [abd]$ thus corresponds to the differential form $dx_a \wedge dx_b \wedge dx_d$.

Extrapolating (and we invite the reader to do so), if we identify cyclic permutations as above, specifically by performing addition modulo cyclic permutations of vertices, for $W_G$ the walk matrix of an arbitrary digraph $G=(V,E)$, the 3-cycle space is given by $\operatorname{Cyc}_3(G)=[\operatorname{tr}(W_G^3)]$

#### The Formula

For $p+q=n$ and $\forall x \in 2^V$ the identity,

$\operatorname{Cyc}_n(G)=\delta_-^p(\epsilon_+(x))\delta_+^q(\epsilon_-(x))\cong [\operatorname{tr}(W_G^n)]$

yields the $n$-cycle space of an arbitrary digraph as the sum of cyclic conjugacy classes, thus producing the set of Hamiltonian cycles as an abstract algebraic combinatorial polynomial.

## **Problem**

If our formula in the **Motivating Case** admits a *polynomial time* implementation by classical Turing machine for arbitrary choice of directed graph, this seems to imply that P=NP...(wtf)

## **Bias 1**

I'm a much better mathematical physicist than mathematician, physicist, or computer scientist.

## **Bias 2**

P=NP doesn't smell right.

## **Statement**

Every family of $n$-dimensional classical statistical-mechanical dynamical systems has a quantum field theoretic counterpart (ie a unitary $L^2$ Hilbert space over an $\mathbb{R}^n$ harmonic, aka conservative, scalar potential) -- the isomorphism relating these models is called a **Wick Rotation**.

## **Proposition 1**

There exists a bijective (unique up to isomorphism) mapping from Turing machines to initial boundary value problems (ie a continuous time dynamical system), using Lagrange interpolation and the Hamiltonian formulation of classical mechanics. This isomorphism is the Wick rotation.

## **Proposition 1.1**

Binary input data can be encoded as Cauchy/Liouville boundary conditions, and the infinite tape criterion replaced by the requirement that the dynamical system be allowed to run an infinite amount of time.

## **Proposition 1.2**

The classical (and quantum) Hamiltonian generates a universal covering space for the orbits of an algorithm's Turing successor function acting on its semi-group's initial conditions

## **Proposition 2**

Given **Proposition 1.X**, the canonical computational complexity is given by the sum of the character of an algorithm's successor function's composition algebra. This is rigorously captured as the sum of the inner product of the Chern character with the Todd index (Atiyah-Singer) of a Turing manifold (to be defined).

## **Prime Fact**

Every physically realizable computer (to date) is subject to conservation of energy, and therefore admits a Hamiltonian description (however complicated)

## **Proposition 3**

Using **Proposition 2**, the **Statement**, and the **Prime Fact**, we can rigorously answer the **Problem** in the **negative** for all possible refactorings (diffeomorphisms) to our formula in the **Motivating Case**, provided the Turing successor function's transition graph is finite.

## Statement of arugment
For input $x$ progression of states $T(x)$ a Turing machine undergoes during it's operation is given by the time series that the machine's successor function $\sigma_T$ generates while operating on the initial data. $T_n = \sigma_T^n(x)$ records the machine's state after $n$ applications of the successor function with initial data $x$. The maximum number of unique states at the $n$-th iteration is n, so $[n] := \{0, 1, 2, ... , n-1\}$ labels the set of all possible states at the $n$-th iteration.

a Turing Machine might take can be represented as the orbit of a dynamical system in the real numbers using Lagrange interpolation. To see this, let  ðœŽ  be a Turing Machineâ€™s transition function over a set of states  ð‘„  with  Î“  the tape alphabet and  ðœŽ:ð‘„Ã—Î“â†’ð‘„Ã—Î“Ã—â„¤2 . A Differential Turing Machine is therefore defined as the oriented flag manifold  ð”[ð‘‹0]  defined by the bundle of  ðœŽð‘˜  orbits of input data  \X0 , for  ð‘˜=1,2,...,ð‘› . Then for each  ð‘›  the equivalence class  [ð±]ð‘›  defines a family of Lagrange polynomials over the finite field  â„¤ð‘› . In the inductive limit, the implementation space of an algorithm is defined by the transport  ð¹  of its combinatorial species, which is invariant under the  ðº -action of  ð‘†ð‘› , the symmetric group on  ð‘›  letters. In other words, the computational complexity of a program will remain the same under permutation of symbols of the formal languageâ€™s alphabet. In this way, the combinatorics of a permutation  ðœ‹:Î“â†’Î“  acting on a countable set determine the fiber bundles of orbits for a dynamical systemâ€™s evolution operator  ðœŽ . In the Almost Finite (AF) limit that  ð‘›â†’âˆž , the space  ð”[ð‘‹0]  has an analytic representation through the one parameter evolution equation  ð‘‘ðœ‹(ð‘¡)ð‘‘ð‘¡=ð‘£(ðœ‹(ð‘¡)) , which by integration has the solution  ðœ‹(ð‘¡)=ðœŽð‘¡(ðœ‹(0)) . From this perspective, the dynamical systemâ€™s phase space is identified with the Turing Machineâ€™s state space, and computation is interpreted as the flow of information across the corresponding phase manifold.

An element  ðœŽâˆˆð‘†ð‘›  is isomorphic to the associated polynomial  ðœŽâ‰…ð¶ð‘›=ð‘0+ð‘1ð‘¥+â‹¯+ð‘ð‘›âˆ’1ð‘¥ð‘›âˆ’1  of a circulant matrix, which itself is defined by the transpose of the Vandermonde matrix of  ðœŽ  orbits. The normalized eigenvectors of the associated polynomial  ð¶ð‘›  are given in terms of the  ð‘› th roots of unity, and furthermore, through the characteristic and associated polynomials of the circulant matrix defined by  ð¶ð‘› , every permutation additionally defines a kernel of the circular convolution. Each Lagrange polynomial has a Vandermonde representation, generating a system of polynomial equations at the multiplicative identity  1âˆˆâ„¤ð‘›  which is solvable by the Chinese Remainder Theorem. This procedure can be inductively applied in the limit that  ð‘›â†’âˆž . By the Cayley-Hamilton Theorem, a matrix solves its characteristic equation, so the product of eigenvalues must equal  1modð‘› , however by definition the  ð‘› th roots of unity are also solutions to the cyclotomic polynomial equation  Î¦ð‘›=ð‘¥ð‘›âˆ’1  with  Î¦ð‘›=0 , and therefore the eigenvalues of the Vandermonde matrix are necessarily algebraic numbers.

Every cyclotomic polynomial  Î¦ð‘›  has an equivalent representation as the tangent bundles of a Differentiable Turing Machine  ð”  using truncated k-jets, and in the inductive limit, this process holds regardless of the indeterminate because the enveloping algebra forms a unitary representation of the cone of modules defined by the Turing Sequence  [ð±]ð‘›  under the action of an element of  ð‘†ð‘› . Thus every effectively computable function has an embedded realization as an orbifold generated by a one parameter semi-group, and the orbits of  ðœŽ  are expressible as a polynomial ring itself having an algebraic p-adic representation, which therefore contains the space of polynomial time solutions.

## **Paradox**

Proving **Proposition 3** exposes an inconsistency in the canonical theory of asymptotic computational complexity:

There exists a physically constructable, *polynomially expressible* formula for a Turing machine which solves an $\mathbf{NP_{HARD}}$ problem, yet has no canonical *polynomial time* implementation, even if the Turing successor function is given a countably infinite number of iterations. This suggests that while it is possible to symbolically express an $\mathbf{NP}$ algorithm as a polynomial, there is no programming language capable of providing a classical $\mathbf{P}$ implementation.

In other words big-O notation is not alone an adequate measure of asymptotic computational complexity because, as shall be shown, complexity itself is relative and scale dependent, necessitating sublter asymptotic measures than summing counts of a successor function -- we must also account for the amount of information each application moves, called the **intrinsic complexity**.

## **Proposition 4**

The *Paradox* is resolved by a process of topological renormalization called *simplification*, rigorously defined later, that involves replacing the monadic Kleene algebra generated by a Turing machine's successor function with a Von Neumann algebra that has identical rules of arithmetic but generates a non-Turing complete grammar, despite preserving intrinsic complexity.

## **Consequence 1**

An algorithm's instrinic complexity class is given by the exponential generating function that defines it.

## **Consequence 2**

Almost Finite rational polynomials correspond to those *polynomial-expressible* Turing orbits that halt after countably infinitely many applications of an algorithm's Turing successor function.

## **Consequence 3**

*Consequence 1* and *Consequence 2* imply that intrinsic complexity has a lower semi-continuous spectrum.

## **Consequence(s) 3.1**

### *Rube Goldberg's Lemma*
You can always make a simple problem harder than it needs to be.

### *Dan's Conjecture*
Sometimes there's no shorter explanation than the journey of discovery

## **Proposition 5**

$\mathbf{HARD_{AF}}$ is the algebraic completion by Dedekind cuts of the space of rational Almost Finite polynomials $\mathbf{A_P}$, and contains the irrational polynomials that uniquely identify, up to a refactoring, those Turing machines that do not halt after countably infinitely many applications of an algorithm's Turing successor function $\sigma$.

## **Consequence 4**

Every Turing machine is identified by the rational or irrational Almost Finite polynomial that defines it.

## ***The* No Free Lunch Theorem**

In light of Noether's theorem applied to an inductively emergent differential symmetry, *Consequence 4* implies $\mathbf{A_P}\cap\mathbf{HARD_{AF}}=\empty$ because intrinsic complexity is a conserved quantity.

## **Lemma**

$\mathbf{A_P}\cap\mathbf{HARD_{AF}}=\empty \implies \mathbf{P}\ne\mathbf{NP}$

## *Proof*

A key piece of rigour is that $\mathbf{A_P}$ and $\mathbf{HARD_{AF}}$ are constructed as G-sets and thus form Cartesian Closed Categories.

The complexity class of decision problems with an irrational Turing Sequence is called $\mathbf{HARD_{AF}}$, where by Dedekind cut construction, $\mathbf{HARD_{AF}}$ can be shown to be the algebraic completion of almost finite rational Turing Sequences. Remember that the set of algorithms with polynomial time complexity $\mathbf{P}$ lies inside the almost finite polynomial ring generated by the rational Turing Sequences. As $\mathbf{HARD_{AF}}$ labels both the complexity class and the space of implementations in this class, it also represents the algebraic closure of the polynomially expressible algorithms $\mathbf{A_P}$, and so necessarily $\mathbf{P}$ and $\mathbf{HARD_{AF}}$ are disjoint.

By definition, $\mathbf{NP}\cap\mathbf{A_P}=\empty$ and because the symmetric difference of disjoint sets equals their union,

$\mathbf{NP}\triangle\mathbf{A_P}=\mathbf{NP}\cup\mathbf{A_P}$.

Similarly, $\mathbf{A_P}\triangle\mathbf{HARD_{AF}}=\mathbf{A_P}\cup\mathbf{HARD_{AF}}$ which implies that

$\mathbf{P}\cap\mathbf{HARD_{AF}}=\empty$

On the other hand, it is clear that $\mathbf{NP}$ and $\mathbf{HARD_{AF}}$ are not disjoint and the symmetric difference of two overlapping sets equals the complement of their intersection, thus

$\mathbf{NP}\triangle\mathbf{HARD_{AF}}=(\mathbf{NP}\cap\mathbf{HARD_{AF}})^C=\mathbf{NP}\cup\mathbf{HARD_{AF}}-\mathbf{NP}\cap\mathbf{HARD_{AF}}$

As the symmetric difference is distributive over set intersection and the operation of intersecting sets is commutative and associative, it therefore follows that 

$\mathbf{P}\cap(\mathbf{NP}\triangle\mathbf{HARD_{AF}})=\mathbf{P}\cap(\mathbf{NP}\cup\mathbf{HARD_{AF}}-\mathbf{NP}\cap\mathbf{HARD_{AF}})$

Reducing,

$\mathbf{P}\cap(\mathbf{NP}\triangle\mathbf{HARD_{AF}})=\empty=\mathbf{P}\cap\mathbf{NP}$

therefore,

$\mathbf{P}\cap\mathbf{NP}=\empty$

In other words, for any algorithm with finite order transition graph, $\mathbf{P}\ne\mathbf{NP}$.

# 1. Paradox as a Laboratory

The importance of Cartesian closed algebras is that they preserve cardinal arithmetic in the inductive limit. Therefore, the first step to obtaining an explanation to the **Motivating Case** is to create a complexity hierarchy that is invariant under the exchange of Cartesian closed algebras. 

Standard category theoretical constructs of set algebras will fail to be Cartesian closed, but transitioning to categories built by simplicial sets -- a
bialgebraic structure -- that shares arithmetical rules with the objects that generate its HOM set, we arrive at a bi-Cartesian closed algebra on the automorphism group. However, bi-Cartesian closed algebras carry, through their inductive completeness, extra information which we will find to be the source of our confusion, necessitating a subtler interpretation and analysis of asymptotic complexity.

We're staring at two sides of infinity across the inner and outer automorphism groups.

A finite demonstration of Russel's Paradox to highlight the pitfalls of canonical asymptotic analysis:

Let $X$ be an $n \times n$ matrix, what is the computational complexity of computing $X^n$?

$X^2 = X * X^1 \cong O(n^3)$

$X^3 = X * X^2 \cong O(n^3)$

$X^4 = X * X^3 \cong O(n^3)$

...

$X^m \cong O(H(m - 1) * n^3) \cong O(n^3)$

for every bounded continuous homomorphism $H$ and $m\lt n$. However in the limit that $m$ goes to $n$ we have

$X^n \cong O(H(n - 1) * n^3) \cong O(n^4-n^3) \ncong O(n^3)$

--> the paradox of **closed** cartesian (bi) categories in the inductive setting, and other examples where inductive completeness leads to seeming contradiction

Demonstrate there exists a common explanation, and resolution.

# 2. Computing With Dynamical Systems

Classical and quantum Hamiltonian dynamics -- computation as the flow of information, and the Hamiltonian therefore is the algebraic generator of time evolution (the current is then identified with the action of the Turing successor fcn). 

* SchrÃ¶der's equation allows us to relate the innner and outer automorphism groups of $\mathbf{a}$ in terms of a dynamical system
* Algorithmic topology requires the simplicial complex to be of finite order.
* The simplicial complex's adjacency matrix $A$ defines the key topological invariant $W$ called the walk matrix

Binary data encoded as the boundary value conditions to a Cauchy-Liouville PDE.

Wick transforms and its connection to the GNS $C^*$ algebra construction, deep clues to a duality relationship

With the infinite tape requirement replaced by an infinite time requirement, my theory demonstrates that a Turing Machine can be modeled as the combinatorial transport, or flow, of information in a dynamical system. When the systemâ€™s phase space defines a conservative vector field, the dissipation of entropy (ala Chaitin) is an algebraically dual description to the flow of information.

<!-- where $N_Z=|Z(T,n)|$ counts the number of partitions $d=\pi_X\in Z$ for the set $T_n=X_{[n]}$. As the transport is a covering of groupoids, the fibers are related by translation functions on coproducts. In this way, the combinatorics of a permutation $\pi_X$ acting on a countable set, determine the fiber bundles of orbits for a dynamical system's evolution operator $\sigma$. For a more detailed treatment of combinatorial species see X. -->

# 3. Information as Deviation From Random

Entropic representation of information ala Chaitin. Dissipation of entropy = flow of information --> fluxuation/dissipation theorem for information.

Information as points vs holes, historical reference to the "Dirac Sea".

Information can be modeled in the dual by considering how it differs from a uniformly random distribution, which has a finite representation in terms of $\mathbf{{Sym(N)}}$, the n-th order symmetric group. The inductive limits of the combinatorial transport may not converge but they can always be embedded in a smooth fashion.

# 4. What Is Intrinsic Complexity?

Topologically speaking, a spoon and a shovel are identical; though with one you eat soup and the other you dig a ditch. Eating soup with a shovel or digging a ditch with a spoon represents an added level of complexity to the original tasks due to using an ineffective tool for the job. To dig the Panama canal, however, the complexity of using a spoon vs a shovel is asymtotically identical -- one person digging a canal the width of Panama is equally inefficient when comparing spoon to shovel.

**Lesson 1:** Complexity is relative to the scale of the problem, which necessarily depends on the tools being used, so any measure of "absolute" complexity needs to be scale and tool invariant. This is to say, the asymptotic behavior of a topologically defined measure of an algorithm's complexity must be unchanged by choice of gauge (ie perspective of scale, or choice of measure).

No matter the method used to dig the Panama canal (given idealized circumstances and an infinite amount of time to complete the task), basically the same amount of dirt needs to be moved regardless of the construction foreman's day to day instructions. Then given the decision problem "can the Panama canal be built", the dirt-to-be-moved represents the problem's "intrinsic complexity", or the difficulty of the task. A consistent and comprehensive theory of complexity must consider both the number of steps to solve a problem and the effort required to complete each step.

**Lesson 2:** Algorithmic topology says that computational complexity is relative but difficulty is conserved.

The expressive power of a programming language interacts with the difficulty of a problem to produce the complexity of an algorithm.

**Lesson 3:** When measuring a decision problem's complexity, algebra is a stronger measure than arithemtic of because it is topologically invariant.

With the infinite tape requirement replaced by an infinite time requirement, Algorithmic Topology posits that a Turing machine can be modeled as the combinatorial transport, or flow, of information in a dynamical system. When the system's phase space defines a conservative vector field, the dissipation of entropy is an algebraically dual description to the dynamics of flowing information.

# 5. The Need For Topology (and scale)

shovel with or without a handle matters when you're digging a grave vs a swimming pool, but not when eating soup or digging the Panama Canal.

# 6. Topological Invariants of Dynamical Systems

Computational gauge theory for Sysyphysean decision problems (infinite tape replaced by infinite time). The singlular homology + limit cycle behavior of Turing machines classifies, up to an isomorphism, those Turing machines that halt after a countable number of applications of the Hamiltonian.

**NOTE 1** this does not contradict the Church-Turing Thesis, we merely have a mathematical model for the information geometry of "long running processes".

**NOTE 2** The same mathematics permitting our far reaching results also exposes an even larger class of problems, with Turing machines that do not halt after countably many applications of its successor function.

# 7. Conservation Laws in Dynamical Systems With Differential Symmetries

Noether's Theorem applied to the long term behavior of trajectories in a Turing orbifold. The dissipation of entropy exactly matches the flow of information when intrinsic complexity is conserved; and that the Turing machine observes energy conservation is **the** necessary condition for a dynamical system to admit a Turing orbifold construction.

# 8. Using Duality To Resolve Paradox

Flow of information --> theory of currents via transverse non-commutative differential geometry allows discontinuous representation + connection to the Wodzicki residue + information and dimensionality.  Rigrously establishes claims from Topological Invariants chapter

Demonstrate that Chaitin's entropic representation of information is an expression of Poincare-Hodge duality on the information geometry.

# 9. A Rose By Any Other Name

NCDG --> Strong Morita Equivalence, exchange of coproducts leave the commutator, and therefore Noether's theorem, invariant.

Permutation in vector space basis labels lead to representations of the infinite symmetric group as an Almost Finite Chern polynomial (theory of infinitessimal refactorings). In other words, an inductively formed differential symmetry. This also means that the symmetry is sponatenously broken locally.

# 10. The Duality Gap

Generalized Legendre transform, relationship to bi-duals in optimization theory.

The dual process to complexification is called **simplification**.

The theory of representations is necessary to rigorously construct the Haar measure in the infinite symmetric group. The infinite symmetric group is the inductive, or almost finite, limit of a dynamical system's iterated composition of its one parameter (time) evolution operator. The Haar measure assigns an invariant volume for analyzing dynamical systems with Lie algebras; and most importantly, it permits the identification of Turing polynomials with a phase manifold through a Noetherian sequence of jet-spaces, independent of an implementationâ€™s language. Once an algorithm is represented in the phase space, we can switch to generalized coordinates and apply Noetherâ€™s Theorem, whose validity extends across different definitions of coproduct so long as both algebras are Cartesian closed, this is the heart of the simplification process, which is the categorical dual of holographic reduction.

If the intrinsic complexity spectrum of physically realizable Turing machines is lower semi-continuous, which it is as a result of local symmetry breaking, there exists a topological obstruction to *simplification*.

Topological renormalization of the "constant overhead" has to take into account the closed Cartesian category phenomena. However, the interpolation of Kleene algebras by Von Neumann algebraic ideals, defines a consistent intrinsic non-abelian computational gauge theory in the context of NCDG -- and importantly -- this explains why all problems can be made harder while other problems cannot be made easier (ie the limits of currying). This is the first clue that P!=NP and hints a possible resolution to the **Paradox**

# 11. Chern Polynomials Encode Intrinsic Complexity

Principle of virtual work leads to equipotentials of complexity, notion of an infinitessimal refactoring as a diffeomorphism that does not alter the intrinsic complexity of an algorithm.

We have a Lie algebroid generated by the operator assigning semi-group evolution equation to Turing orbit, thus generating a groupoid over the initial conditions. Exponentiating the dependent variable in characteristic polynomial of this operator identifies every evolution equation with a Chern polynomial.

A Chern polynomial of the configuration space has a Fredholm module representation, which is isomorphic to Chaitin's entropic representation of information through algebraic embeddings in the symmetric group -- and in the dual representation, intrinsic complexity is therefore proportional to the effort to change a Turing machine's orbit. (link to gravitation and spontaneous symmetry breaking)

# 12. Intrinsic Complexity is a Conserved Quantity

So while canonical asymptotic computational complexity might not be absolute, intrinsic complexity is a conserved quantity and its integral with respect to time is a better measure of total complexity because it captures the entire effort necessary to compute the solution if given an infinite amount of time.

# 13. Interlude On the Absurdity of P=NP (and why it doesn't)

Turing machines that would halt in polynomial time are contained in the space of Turing machines that halt after countably many steps (ie polynomially-expressible time). If no refactoring will produce a Turing machine halting in fewer steps while conserving intrinsic complexity, the proposed algorithm is not a P reduction of an NP problem.


# Mathematical Preliminaries

## 14. Abstract Nonsense And The Lambda Calculus

<!-- Let $\mathbf{B}$ represent the groupoid of finite sets and bijections, with $\mathbf{Set}$ as both the category of sets and functions, a species of structures is given by any functor,

$F : \mathbf{B}\longrightarrow\mathbf{Set}$

For a subgroup $\mathbf{A}$ of $\mathbf{B}$, we let $\mathbf{A}$ represent both the complexity class and the space of algorithms therein definable, to which we assign the **algorithmic topology**, formally defined later in the paper. Following the construction of \cite{baez2001finite}, for a topological space $\mathbf{A}$ we form the category of algorithms, given by

$\Pi_1(\mathbf{A})$ and $F:\mathbf{A}\to \mathbf{A^\prime}$

where the objects of $\Pi_1$ are the implementations of an algorithm $\mathbf{a}\in\mathbf{A}$ and whose morphisms $f$ are the homotopy classes of diffeomorphisms, called *refactorings*, taking $\mathbf{A}$ to $\mathbf{A^\prime}$. With $F$, the category $\Pi_1(\mathbf{A})$ forms the fundamental groupoid of an algorithm $\mathbf{a}$ and it follows that if we decategorify $\Pi_1(\mathbf{a})$, we obtain the set of path components $\Pi_0(\mathbf{a})$ as the succession of states of the implementation's Turing transfer function. Letting $\mathbf{A^\prime}=\mathbf{A^{-1}}$, the endofunctor $f:\mathbf{A}\to \mathbf{A^{-1}}$ generates the automorphism group $aut(\mathbf{A})$, which will provide a coordinate free, and thereby language independent, representation of $\mathbf{a}$.

An important theme of this paper is that when studying complexity, one must pay close attention to the effect algebra has on arithmetic. To that end, when enumerating the sequence of states $T_k$ produced by a Turing Machine's transfer function, we gain much by first formalizing the process of uniquely labeling a set's elements.

Let $X$ be a countable set of $n=|X|$ elements. With $\mathbb{Z}_n=\mathbb{Z}\setminus n \mathbb{Z}$ the bijective indexing function $\pi_X:\mathbb{Z}_n\rightarrow X$ is defined by $i\mapsto x_i=\pi_X(i)$, which acts to index the elements of $X$ with the integers modulo the cardinality of $X$. That $\pi_X$ must be bijective is a requirement if we are to uniquely label the $n$ elements of $X$; and is a consequence of the pigeonhole principle for the integers modulo $n$, because any domain with a different number of equivalence classes of elements would not be both one to one and onto. These considerations are important since we've only specified that $X$ is countable, not necessarily finite. For countable $X$ and its powerset $2^X$, let

$[n]=\{1,2,...,n-1\}$, & $X_{[n]}=\{X_{[1]},X_{[2]},...,X_{[n-1]}\}\in 2^X$

with $X_{[0]}=\emptyset$, $X_{[1]}=X$ and $X_{[n-1]}\subseteq X_{[n]} \subset 2^X$. The quantity $[n]$ then indexes the equivalence class of partitions $Z(X,n)=X_{[n]}$ satisfying these properties. Since $X_{[n-1]}\subseteq X_{[n]}$, the inclusion map (canonical injection) is the function

$\iota : X_{[n-1]}\to X_{[n]}$ such that $\iota(X)=X$

which is written $\iota : X_{[n-1]}\hookrightarrow X_{[n]}$.

The implementations of an algorithm are given by the functors $F\in\mathbf{A}$ as determined by their action after a countable number of steps. Specifically, let $\mathbf{A}$ be the full subgroupoid of $\mathbf{B}$ whose objects are the finite cardinals and Hom-sets are given by

$\mathbf{A}[u,v]=\left\{{\begin{matrix}S_{[n]}&if\ u=v=[n] \\ \emptyset &{otherwise}\end{matrix}}\right.$

with $S_{[n]}$ as the symmetric group over the set $[n]$. Computing the composition of implementations with the canonical injection $\iota$, the space of implementations of an algorithm acts as $\mathbf{A}\hookrightarrow \mathbf{B}$; which yields an equivalence of functor categories $[\mathbf{B},\mathbf{Set}]\cong [\mathbf{A}, \mathbf{Set}] $ given by the congruence $\mathbf{B}\longrightarrow\mathbf{Set} \cong \mathbf{A} \longrightarrow\mathbf{Set}$ -->

<!-- admitting the following diagram -->
<!-- $\xymatrix{
\mathbf{A}[u,v] \ar[d]_{[u,\delta]} \ar[r]^{[\epsilon,v]} & \mathbf{A}[u^\prime,v] \ar[d]^{[u^\prime,\delta]}\\
\mathbf{A}[u,v^\prime] \ar[r]_{[\epsilon,v^\prime]} & \mathbb{A}[u^\prime,v^\prime]}$ -->
<!-- 
In the inductive limit, the transport of $F[X]=\pi_X$ has an associated flag manifold through partial decategorification in terms of its exponential generating function,

$F(\pi_X(t)) = \lim_{N_Z\rightarrow\infty}\sum_{n\ge 0}^{N_Z}N_Z\frac{\sigma^t}{n!}$

where $N_Z=|Z(T,n)|$ counts the number of partitions $d=\pi_X\in Z$ for the set $T_n=X_{[n]}$. As the transport is a covering of groupoids, the fibers are related by translation functions on coproducts. In this way, the combinatorics of a permutation $\pi_X$ acting on a countable set, determine the fiber bundles of orbits for a dynamical system's evolution operator $\sigma$. For a more detailed treatment of combinatorial species see X. -->

## 15. Constructing A Vector Space From A Formal Language

<!-- Intuitively speaking, we've created a model of computation where every Turing recognizable algorithm is represented as information flowing, or equivalently as entropy dissipating, across the state space manifold $\mathcal{M}_{L^*}$, which contains the orbits of a Turing Machine's transfer function \cite{vignati2017logic}, with traces through a program's control flow given by the trajectory of the successor function $\sigma\in S_{|\Gamma|}$ on the initial data $X_0$. Note that $\chi_{accept}\subseteq L^*_{\sigma}(\Gamma\otimes Q)(X_0)$, so the realization of the Turing Polynomial embeds the state transition function in a differential Turing machine $\mathfrak{M}[X_0]$ as an almost finite subspace of $\pi^*$. This implies that the rational Turing orbits have an algebraic p-adic representation, and therefore enumerate the space of polynomial time solutions.

Through the analytic embedding of Turing Machines into differentiable manifolds, the space of accepting and rejecting states $\chi_{accept}$ and $\chi_{reject}$ partition $\mathfrak{M}[X_0]$ and its Grothendieck universe \cite{vershik1990grothendieck} into closed and open sets, respectively.
While theoretically compelling, for this proscription to have any practical use, it remains to be shown that there exists a reasonably simple procedure for embedding an arbitrary algorithm in an arbitrary formal language.

\subsection{Formal Language}

A language $L_X$ over alphabet $X$ is a subset of $2^X$, the power set of $X$. The set of all strings in $L_X$ not containing the empty string $\lambda$ is given by the Kleene Plus $L^+_X = 2^{X \setminus \lambda}$. Taking string concatenation to be multiplication, the multiplicative closure of $L_X$ is given by the Kleene Star $L^*_X = L^+_X \cup \{\lambda\}$. $L^*_X$ is then the free monoid over $X$ and a multiplicative semi-ring over $2^X$.

\begin{theorem}
With the formal sum ($+$) as the coproduct and intersection as multiplication, $L^*_X$ forms a graded vector space.
\end{theorem}

\begin{proof}
stuff
\end{proof}

\begin{lemma}
With the symmetric difference ($\ominus$) as the coproduct and intersection as multiplication, $L^*_X$ forms a graded vector space.
\end{lemma}

\begin{proof}
stuff
\end{proof}

To build intuition, we initially regard a directed graph as the pullback of some functional $L:E\rightarrow \mathbb{N}$ and then construct a formal language $L_V$ from the space of edge sequences, which we identify with paths. An abstract graph $G=(V,E)$ can be thought of as the mapping $G: V \mapsto E$. For $n=|V|$ let the set of edges $E$ define a formal language over the vertex set $V$, then $G \cong (\pi_V^{-1},L_{\pi_V})$ and we may work in $V\cong \mathbb{Z}_n$ and $E \cong [i\wedge j]\ \forall i,j\in\mathbb{Z}_n$ without loss of generality. The use of the wedge product is intentional, as we are operating on the subspace of a Grassman Algebra\footnote{See \cite{wolf1965translation} for an overview of Grassman Algebras} inherited from the index quiver representation of $\pi_V$\footnote{This follows from the mathematics of partition algebras, and the reader is again referred to \cite{Halverson05partitionalgebras}}. By summing over the elements of a semi-ring subset of the monoid generated by a formal languageâ€™s Kleene Algebra. Steps in a trace of the implementation's control/flow are given by sequences of the form $\{(x_m)|x_0\in X_0\}$, where any two contiguous steps related through the inner and outer successor functions.

\begin{definition}[Simple Path]
A path is called simple if it is acyclic and has zero or more loops.
\end{definition}

\begin{theorem}
The equivalence class of irreducible substring products which comprise a word is the space of simple paths $\Xi$.
\end{theorem}

\begin{proof}
For a flow algebra $\Xi(E)$, the space of simple paths forms the two sided ideal of $\Xi$. A two sided ideal gives the center of a group, which by definition contains the left and right multiplicative equivalence classes as cosets.
\end{proof}

\begin{theorem}
$\Xi \cong Gr(n,\mathbb{C})$
\end{theorem}

\begin{proof}
An $m$-character string defining a word $x \in \Xi(E)$ has a corresponding representation in terms of the wedge product of basis vectors $x_i$ canonically ordered by a permutation $\pi\in S_n$. As Turing Polynomials are defined using the tensor product, any element $x_{\pi(1)} \wedge ... \wedge x_{\pi(k)} \wedge ... \wedge x_{\pi(m)}$ for $x_k \in V$ will span an $m$-dimensional subspace of the $n$-dimensional complex Grassman space. Thus we may represent the simple paths by their character vector in Grassman coordinates.
\end{proof}

\begin{definition}[Cycle and loop]
A path $x$ is a cycle if $t(x)=h(x)$ and the cycle space of a language is denoted $\mathbf{Cyc}(L_V)$. A loop is an element of $\mathbf{Cyc}(L_V)$ embedded as a \emph{strict} sub-path. A trivial path is a self loop, or a word $a \in L_{\pi_V}$ of the form $aa$.
\end{definition}

\begin{definition}[Reversal operator]
The reversal operator $\hat{r}:2^E \rightarrow 2^E$ replaces the sign of a permutation of elements with the orientation of a path. It is recursively defined by
\begin{equation}
\hat{r}^k(x,y)=(y,\hat{r}^{k-1}x)
\end{equation}
with $\hat{r}^0=\mathbf{1}$. We further specify that $x\wedge y = \hat{r}y\wedge x$.
\end{definition}

\begin{theorem}
Elements of the cycle space $\mathbf{Cyc}(L_V)$ are fixed points of $\hat{r}$.
\end{theorem}

\begin{proof}
Let $x$ be a path in the flow algebra $\Xi(E)$, then $x_0=t(x)$ and $x_n=h(x)$. If $x$ is a cycle then $t(x)=h(x)$ which implies that the head and tail of $x$ are identical. Note that $t\hat{r}(x)=h(x)=x_i$ and $h\hat{r}(x)=t(x)$, so elements of the cycle space $\mathbf{Cyc}(L_V)$ are the fixed points of the reversal operator.

then for $x_0=t(x)$ and $x_i=h(x)$ then $x\in\mathbf{Cyc}(L_V)$...etc
\end{proof}

\begin{theorem}
With $n=|V|$ and $x,y\in\Xi(E)$, the Grothendieck Group of $\Xi(E)$ is given by the identification $x-y=x\hat{r}y$ and $L_{\pi^*} (\mathbb{C}) = \Xi(L_{\mathbb{R}})$ in the inductive limit.
\end{theorem}

\begin{proof}
As $\Xi(E)$ defines a commutative monoid, its Grothendieck Group $K$ is defined by a monoid homomorphism $i:M\rightarrow K$ where $f:M\rightarrow A$ from $\Xi(E)$ to an abelian group $A$. There is a unique group homomorphism $g:K\rightarrow A$ such that $f=g\circ i$. Let G be an abelian group, then $K(G)\cong G$ via the identification $(g,\hat{r}h)\sim gh^{-1}$. As $g$ and $h$ represent permutations, the inverse exists and is defined through function composition. Being multiplicatively closed, for each of $\mathbb{Z}_\mathbf{n}$ in inductive limit there exists a language $L^*_{\mathbb{Z}_{\mathbf{n}-2}}$ with an embedding given by $\Xi(L_{\mathbb{Z}_{\mathbf{n}-2}})\cong \Xi(L_{\mathbb{T}^{\mathbf{n}}})$. Therefore,
\begin{equation}
\lim_{n\rightarrow\infty}\Xi(L_{\mathbb{T}^{\mathbf{n}}})=\Xi(L_{\mathbb{R}})
\end{equation}
and the equivalence class of elements of $\Xi(L_{\mathbb{R}})$ is given by,
\begin{equation}
L_{\pi^*} (\mathbb{T}^\infty) = \Xi(L_{\mathbb{R}})
\end{equation}
for the full group. However $(\mathbb{T}^\infty)\cong\mathbb{C}$ represented as the complex p-adic numbers.
\end{proof}

\begin{lemma}
\begin{equation}
x^a\hat{r}x^b = (-1)^{\operatorname{sgn}(x)}(a-b)x^{a+b}
\end{equation}
\end{lemma}

\begin{proof}
This follows directly from the Umbral calculus for reversing linear functionals \cite{ROMAN197895}.
\end{proof}

\begin{theorem}
A non-trivial path is either cyclic or acyclic.
\end{theorem}
\begin{proof}
Let $x\in\Xi(L_{\pi_V})$ be a non-trivial path and suppose $x$ is both cyclic and acyclic. If the path defines a cycle then $t(x)=h(x)$, however if the $x$ is acyclic, necessarily $t(x)\ne h(x)$, a contradiction.
\end{proof} -->

## 16. Adjacency and Walk Matrix generate a Turing monoid's Transition space

<!-- Given a directed graph $G_V=(V, E)$ with vertex and edge sets $V$ and $E$ respectively, the following statements about $G$'s adjacency matrix $A=[a_{ij}]$ are true:

* $a_{ij}\in \mathbb{Z}_2$ for $0\leq i,j \leq |V|$
* The matrix $A$ uniquely defines a rank $|V|$ permutation matrix up to isomorphism
* For $A^k=[x_{ij}]$, element $x_{ij}$ equals the number of walks of length $k$ from the vertex labeled $i$ to the vertex labeled $j$
* For $n = |V|$, the directred graph acts as the parameterized functor $G_V : \mathbb{Z}_{n} \mapsto E$

The Cartesian product $V\times V$ is isomorphic to a subset $U\subseteq E$. Let $A$ also represent the function $A: U\to V\times V$ such that $A(i, j) = a_{ij}$.

Now consider the indicator function

As each directed graph is uniquely represented, up to isomorphism, by its adjacency matrix, let $\mathbf{G}$ be the set of all adjacency matrices. Given a graph $G=(V,E)$ with adjacency matrix $A$,

$\mathbf{A}=
    \begin{pmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \dots & \vdots \\
        a_{n1} & a_{n2} & \dots & a_{nn}
    \end{pmatrix}$

and $\chi:\mathbf{G} \to GL[2^V]$ acts element-wise where $(2^V)^{ij}:(i,j)\mapsto E\oplus \mathbf{0}$

$\mathbf{W} = \begin{pmatrix}
    \chi^{11}(1,1) & \chi^{12}(1,2) & \dots & \chi^{1n}(1,n) \\
    \chi^{21}(2,1) & \chi^{22}(2,2) & \dots & \chi^{2n}(2,n) \\
    \vdots & \vdots & \ddots & \vdots \\
    \chi^{n1}(n,1) & \chi^{n2}(n,2) & \dots & \chi^{nn}(n,n)
\end{pmatrix}$

An element $\sigma\in S_{[n]}$ is isomorphic to the associated polynomial of a circulant matrix,

$\sigma \cong C_n = c_0 + c_1 x + ... + c_{n-1}x^{n-1}$

defined by a composition of cyclic permutation of the transposed Vandermonde matrix of $\sigma$ orbits, 
$C_n={\begin{bmatrix}c_{0}&c_{{n-1}}&\dots &c_{{2}}&c_{{1}}\\c_{{1}}&c_{0}&c_{{n-1}}&&c_{{2}}\\\vdots &c_{{1}}&c_{0}&\ddots &\vdots \\c_{{n-2}}&&\ddots &\ddots &c_{{n-1}}\\c_{{n-1}}&c_{{n-2}}&\dots &c_{{1}}&c_{0}\\\end{bmatrix}}$

The normalized eigenvectors of the associated polynomial are given in terms of the $n$th roots of unity,

$v_k = \frac{1}{\sqrt{n}}(1,\omega_k,\omega^2_k,...,\omega^{n-1}_k)$, for $k=0,1,...,n-1$

where $\omega_k=\operatorname{exp}(\frac{2\pi i k}{n})$ and eigenvalues,

$\lambda_k = c_0 + c_1 \omega_k + ... + c_{n-1}\omega^{n-1}_k$

Letting $c_k=\sigma^k$ the eigenvalues have the alternate representation,
$\lambda_k = \sigma^0 + \sigma^1 \omega_k + ... + \sigma^{n-1}\omega^{n-1}_k$

$\lambda_k = c_0 + c_1 \omega_k + ... + c_{n-1}\omega^{n-1}_k$

Letting $c_k=\sigma^k$ the eigenvalues have the alternate representation,
$\lambda_k = \sigma^0 + \sigma^1 \omega_k + ... + \sigma^{n-1}\omega^{n-1}_k$

$\lambda_k = c_0 + c_1 \omega_k + ... + c_{n-1}\omega^{n-1}_k$

Letting $c_k=\sigma^k$ the eigenvalues have the alternate representation,
$\lambda_k = \sigma^0 + \sigma^1 \omega_k + ... + \sigma^{n-1}\omega^{n-1}_k$ -->

## 17. Algebraic Statistics and Algebraic Varieties of Programming Languages

<!-- Yang Baxter and set theoretic equivalents -->

## 18. The Lie Algebras of Turing Manifolds 

# Applications

## 19. Analyzing The Relationship Of An Implementation's Cyclomatic To An Algorithm's Intrinsic Complexity

<!-- Under construction -->

## 20 .Flow Algebra For Database Schema Analysis of Foreign Key Relationships

k-nearest neighbor analysis / derive partition function

## 21. The Fundamental Limits Of Currying

Discuss in the context of Eckmann-Hilton duality <--> equivalent to the requirement that flow algebras be associatively embeddable

ie $(1,2)(2,3)(3,4)(4,5) \cong (1,2,3,4,5)$

Demonstrate relationship to process of topological simplification

## 22. Classifying High Entropy Alloys By Intrinsic Complexity

Under construction

## 23. Self Organizing Maps For Predictive Design of Superparamagnetic HEA

Self organized maps preserve the topological invariants of a dataset. With a intrinsic complexity as a key ML feature

## 24. Algorithmic Topology as a foundation for HEA Solid State Spintronic Computers

Using flow algebra, we will analyze the theoretical efficiency of high entropy alloys for computing and encoding the GCD of two numbers as coordinates in a Stern-Brocot tree

## 25. Computing With Waste Heat -- Thermodynamic HEA Spintronic Computers

The key isomorphism is the Wick rotation, which maps a qft Hamiltonian to an equivalent classical thermodynamic Hamiltonian (Poisson bracket equal to the Legendre transform of the Lagrangian). Superparamagnetic materials change their local magnetic structure (read spin structure) based on thermal input. A superparamagnetic HEA as a computing medium would serve to implement the Wick rotation as a hardware process.

## 26. Fundamental Limits On The Entanglement Density Of Near Macroscopic Thermodynamic Systems

Under construction

## 27. Quantum Gravity Is A Theory Of Computation

Under construction